Hi Rathana — good morning! ABC and XYZ shared your contact number.

We’re currently working on the XYZ application and performing data migration from Global to IN. Our current approach is to collect all IN branch codes (including 999) from Global and insert those records into IN.

However, we’re running into file size limitations. Previously, the pipeline executed successfully when we split the data into multiple files of around 10 MB each (total ~50 MB). But now our dataset is much larger — about 950 MB — and when we tried processing files over 100 MB, we encountered OOM (Out Of Memory) errors.

For the PTE environment we are experimenting, but for production the data size is expected to be around 2.5 GB, so we want to ensure we choose the right migration strategy.

Could you please advise on the best approach or recommended strategy for handling large-volume data migration in this scenario?

Thanks in advance for your guidance.
